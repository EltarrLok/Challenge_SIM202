V1(-1,3.2,-2,8)
V1=c(-1,3.2,-2,8)
V2=c(-2,-1,0,1,2,3,4,5,6)
V3=(0.05,0.1,0.15,0.2)
V3=c(0.05,0.1,0.15,0.2)
V4=c(1,1,1,1,1,1,1,1,1,1)
V5=c(OUI,NON)
V5=c('OUI'','NON'')
V5=c('OUI','NON')
V5
sort(V1)
V1
1+2
1+2.0
1.0+3.0
V2
V3
Vinty=c(1,1.0,2.0,2)
Vinty
V6=2*V1-3
V6
V6=2*V2-3
V6
V3+V2
cos(0)
log(1)
log(V1)
V5+1
log(V3)
V5(1)
V5[1]
V5[2]
length(6)
length(V6)
length(V1)
10:(-1)
seg(1,20,by=0,8)
seq(1,20,by=0,8)
seq(1,20,by=0.8)
seq(1,20,length=13)
V6[c(6,1,9)]
V6
V6[c(length(V6):)]
V6[c(length(V6)-3:)]
V6[c(length(V6)-3):]
c(length(V6)-3):
4
V1[1,4]
V1[1]
V6[c(length(V6)-2, c(length(V6)-1, c(length(V6)]
cumsum(V6)
V6
mean(V6)*length(V6)
sum(V6)
length(V2[V2>0])
?choose
n=10;p=0.2
k= 0:10     # ? compl?ter
pk= choose(n,k)*p^k*(1-p)^(n-k)
pk
k
mean(pk)
var(pk)
esperance_attendu = n*p
esperance_attendu
esperance = mean(pk)
esperance
var_attendu = n*p*(1-p)
var_obt = var(pk)
var_obt
var_attendu
Pk=cumsu(pk)
Pk=cumsum(pk)
Pk
df= data.frame(k,proba=pk)
print(df)
df$proba
df["proba"]
df[1:4,2]
df$proba[1:4]
max(df$proba)
summary(df)
attach(df) # acc?s direct aux colonnes, mais attention !!
detach(df)
?summary
?ppois
?pois
pwd
getwd()
setw(dir ="Dossier Personnel")
ls
N=3;
require(bbmle)
cauchy1(scale.arg = 1, llocation = "identitylink", ilocation = NULL,
imethod = 1, gprobs.y = ppoints(19), zero = NULL)
help
View(df)
V1=c(-1,3.2,-2,8)
V2=-2:6
V3=seq(0.05,0.2,0.05)
V4=rep(1,10)
V5=c("OUI","NON")
sort(V1)
# op?ration composante par composante et recyclage
V6=2*V2-3
V6=2*V2-3
V1=c(-1,3.2,-2,8)
V2=-2:6
V3=seq(0.05,0.2,0.05)
V4=rep(1,10)
V5=c("OUI","NON")
sort(V1)
# op?ration composante par composante et recyclage
V6=2*V2-3
V6
V3+V2
log(V3)
log(v6)
log(V6)
V5+1
V5[2]
V7=V6[length(V6)-3:length(V6)]
V7
v7=tail(V6,3) #plus simple !
v7
v7=V6[length(V6)-(2:0)]
v7=V6[length(V6)-(2:0)]
v7
rm(list=objects())
library(tidyverse)
library(lubridate)
library(ranger)
Data0 <- read.csv(file="Data/train.csv", sep=",", dec='.')
Data1 <- read.csv(file="Data/test.csv", sep=",", dec='.')
Data0 <- read.csv(file="train.csv", sep=",", dec='.')
rm(list=objects())
library(tidyverse)
library(lubridate)
library(ranger)
Data0 <- read.csv(file="train.csv", sep=",", dec='.')
install.packages("ranger")
install.packages("glmnetcr")
library(glmnet)
library(glmnet)
library(glmnet)
install.packages(glmnet)
library(glmnet)
setRepositories()
setRepositories()
install.packages("glmnet")
setRepositories(ind = c(1:6, 8))
0
0
setRepositories()
rm(list = objects())
graphics.off()
setwd("/home/lokmen/Documents/ENSTA/SIM202/building-appliances/")   # set working directory
source("rmse.R")
library(readr)
library(mgcv) # modélisation non linéaire
library(glmnet)
library(tidyverse)
library(lubridate)
library(ranger)
#data <- read_delim("train.csv", col_names = TRUE, delim = ",")
#data_test<- read_delim("test.csv", col_names = TRUE, delim = ",")
data0<-read.csv(file="train.csv", sep=",", dec='.')
data_test<-read.csv(file="test.csv", sep=",", dec='.')
# On se débarasse des variables qui sont des facteurs. On les voit avec la commande str()
str(data0)
data_num <- subset( data0, select = -c(date,WeekStatus,Day_of_week,DayType, InstantF))
# On cherche les coeffictions fortement correlés à Appliencies
# on peut observer qu'il y a beaucoup de NA
cor(data_num)[1,]
cor(data_num)[,1]
length(data0)
for (i in c(3;length(data_num))){
data_num[is.na(data_num[,i]), i] <- mean(data_num[,i], na.rm = TRUE)
}
for (i in c(3:length(data_num)))
{
data_num[is.na(data_num[,i]), i] <- mean(data_num[,i], na.rm = TRUE)
}
which(is.na(data0$RH_6))
which(is.na(datadata_num$RH_6))
which(is.na(data_num$RH_6))
library(plyr)
library(zoo)
install.packages("zoo")
install.packages("plyr")
library(plyr)
# On cherche les coeffictions fortement correlés à Appliencies
# on peut observer qu'il y a beaucoup de NA
cor(data_num)[1,]
which(is.na(cor(data_num)[,1]))
data_num <- subset( data0, select = -c(date,WeekStatus,Day_of_week,DayType, InstantF))
which(is.na(cor(data_num)[,1]))
ddply(data_num, transform, RH_6=na.approx(BM, rule=2))
ddply(data_num, transform, RH_6=na.approx(RH_6, rule=2))
data_num <- subset( data0, select = -c(date,WeekStatus,Day_of_week,DayType, InstantF))
na.approx(data_num)
library(zoo)
na.approx(data_num)
# On voit que ça a bien fonctionneé
which(is.na(data0$Visibility))
data_num<-na.approx(data_num)
library(glmnet)
cor(data_num)[1,]
#data <- read_delim("train.csv", col_names = TRUE, delim = ",")
#data_test<- read_delim("test.csv", col_names = TRUE, delim = ",")
data0<-read.csv(file="train.csv", sep=",", dec='.')
data_test<-read.csv(file="test.csv", sep=",", dec='.')
data_num <- subset( data0, select = -c(date,WeekStatus,Day_of_week,DayType, InstantF))
#On utilise na.approx qui est dans me package zoo pour remplacer les NA par interpolation
data_num<-na.approx(data_num)
which(is.na(data_num))
cor(data_num)[1,]
which(is.na(data_num$BE_wind_onshore_capacity))
data_num$BE_wind_onshore_capacity))
cor(data_num)[1,]
data_num$BE_wind_onshore_capacity
data_num
#On utilise na.approx qui est dans me package zoo pour remplacer les NA par interpolation
data_num<-data.fram(na.approx(data_num))
str(data_num)
#On utilise na.approx qui est dans me package zoo pour remplacer les NA par interpolation
data_num<-data.frame(na.approx(data_num))
str(data_num)
which(is.na(data_num$BE_wind_onshore_capacity))
cor(data_num)[1,]
max_corel<-max(cor(data_num)[1,2:])
cor(data_num)
cor(data_num)[1,2:]
cor(data_num)[1,2:]
cor(data_num)[1,2:8]
cor(data_num)[1,2:ncl(data_num)]
cor(data_num)[1,2:ncol(data_num)]
max_corel<-max(cor(data_num)[1,2:ncol(data_num)])
for (i in c(ncol(data_num):2)){
if (cor(data_num[1,i])< 0.25*max_corel)
{
data_num<-data_num[-i]
}
}
for (i in c(ncol(data_num):2)){
if (cor(data_num)[1,i]< 0.25*max_corel)
{
data_num<-data_num[-i]
}
}
cor(data_num)[1,7]
for (i in c(ncol(data_num):2)){
if (cor(data_num)[1,i]< 0.25*max_corel)
{
data_num<-data_num[-i]
}
}
print(i)
for (i in c(ncol(data_num):2)){
if (cor(data_num)[1,i]< 0.25*max_corel)
{
print(i)
}
}
for (i in c(ncol(data_num):2)){
print(i)
if (cor(data_num)[1,i]< 0.25*max_corel)
{
data_num<-data_num[-i]
}
}
for (i in seq(ncol(data_num):2,-1)){
print(i)
if (cor(data_num)[1,i]< 0.25*max_corel)
{
data_num<-data_num[-i]
}
}
for (i in c(2:ncol(data_num))){
if (cor(data_num)[1,ncol(data_num)-i]< 0.25*max_corel)
{
data_num<-data_num[-(ncol(data_num)-i)]
}
}
if (cor(data_num)[1,2]< 0.25*max_corel)
{
data_num<-data_num[-(ncol(data_num)-2)]
}
cor(data_num)[1,2]
cor(data_num)[1,2]< 0.25*max_corel
is(cor(data_num)[1,2]< 0.25*max_corel)
bol =cor(data_num)[1,2]< 0.25*max_corel
0.25*max_corel
max_corel<-max(cor(data_num)[1,2:ncol(data_num)])
max(cor(data_num)[1,2:ncol(data_num)])
cor(data_num)[1,2:ncol(data_num)]
?max
max_corel<-max(cor(data_num)[1,2:ncol(data_num)], na.rm=TRUE)
cor(data_num)[1,2:ncol(data_num)]
max_corel<-max(cor(data_num)[1,2:ncol(data_num)], na.rm=TRUE)
for (i in c(2:ncol(data_num))){
if (cor(data_num)[1,ncol(data_num)-i]< 0.25*max_corel)
{
data_num<-data_num[-(ncol(data_num)-i)]
}
}
for (i in c(2:ncol(data_num))){
print(i)
if (cor(data_num)[1,ncol(data_num)-i]< 0.25*max_corel)
{
data_num<-data_num[-(ncol(data_num)-i)]
}
}
cor(data_num)[1,ncol(data_num)-2]< 0.25*max_corel)
cor(data_num)[1,ncol(data_num)-2]< 0.25*max_corel
bol =cor(data_num)[1,ncol(data_num)-2]< 0.25*max_corel
1<0
cor(data_num)[1,ncol(data_num)-2]
cor(data_num)[1,2]
ncol(data_num)
ncol(data_num)-2
cor(data_num)[1,2]
cor(data_num)[1,36]
cor(data_num)[1,35]
cor(data_num)[1,]
which(is.na(data_num$BE_wind_onshore_capacity))
#il y a une correaltion sous forme de NA. On la retire
data_num <- data_num[-tail(data_num,1)]
tail(data_num,1)
#il persiste une correaltion sous forme de NA. On la retire
data_num <- subset( data0, select = -BE_wind_onshore_capacity)
max_corel<-max(cor(data_num)[1,2:ncol(data_num)], na.rm=TRUE)
max_corel<-max(cor(data_num)[1,2:ncol(data_num)])
#il persiste une correaltion sous forme de NA. On la retire
data_num <- subset( data0, select = -BE_wind_onshore_capacity)
cor(data_num)
data_num
rm(list = objects())
graphics.off()
setwd("/home/lokmen/Documents/ENSTA/SIM202/building-appliances/")   # set working directory
source("rmse.R")
library(readr)
library(mgcv) # modélisation non linéaire
library(tidyverse)
library(lubridate)
library(ranger)
library(zoo)
#data <- read_delim("train.csv", col_names = TRUE, delim = ",")
#data_test<- read_delim("test.csv", col_names = TRUE, delim = ",")
data0<-read.csv(file="train.csv", sep=",", dec='.')
data_test<-read.csv(file="test.csv", sep=",", dec='.')
data_num <- subset( data0, select = -c(date,WeekStatus,Day_of_week,DayType, InstantF))
#On utilise na.approx qui est dans me package zoo pour remplacer les NA par interpolation
data_num<-data.frame(na.approx(data_num))
data_num
cor(data_num)[1,2:ncol(data_num)]
#il persiste une correaltion sous forme de NA. On la retire
data_num <- subset( data_num, select = -BE_wind_onshore_capacity)
max_corel<-max(cor(data_num)[1,2:ncol(data_num)])
for (i in c(2:ncol(data_num))){
print(i)
if (cor(data_num)[1,ncol(data_num)-i]< 0.25*max_corel)
{
data_num<-data_num[-(ncol(data_num)-i)]
}
}
data_num <- subset( data0, select = -c(2:10))
data_num
data_num <- subset( data0, select = -c(date,WeekStatus,Day_of_week,DayType, InstantF))
#On utilise na.approx qui est dans me package zoo pour remplacer les NA par interpolation
data_num<-data.frame(na.approx(data_num))
#il persiste une correaltion sous forme de NA. On la retire
data_num <- subset( data_num, select = -BE_wind_onshore_capacity)
max_corel<-max(cor(data_num)[1,2:ncol(data_num)])
liste_petit)list()
liste_petit<-list()
for (i in c(2:ncol(data_num))){
print(i)
if (cor(data_num)[1,ncol(data_num)-i]< 0.25*max_corel)
{
list.append(list_petit,i)
}
}
liste_petit<-list()
for (i in c(2:ncol(data_num))){
print(i)
if (cor(data_num)[1,i]< 0.25*max_corel)
{
list.append(list_petit,i)
}
}
liste_petit<-list()
for (i in c(2:ncol(data_num))){
print(i)
if (cor(data_num)[1,i]< 0.25*max_corel)
{
list.append(liste_petit,i)
}
}
list.append(liste_petit,4)
x <- list(a=1,b=2,c=3)
list.append(x,d=4,e=5)
